# CSE 512: Machine Learning (Project)
## Contrasting Methods for Avoiding Overfitting in Neural Networks

### Introduction
This study proposes to compare various methods for preventing overfitting in deep learning neural network models. A deep neural network is replete with parameters and is excessively complex, tends to overfit the data, and thus performs well on the training set that the model had seen but doesn't on the test set that it hasn't seen. Regularization can be defined as any modification or supplementary technique that can be used to improve the results on a test set by making the model generalize better. To prevent overfitting, a number of approaches can be utilized, including Data Augmentation, Early Stopping, L1, L2, Dropout Regularization, or a combination of all of them. In order to determine which combination of strategies would yield the best results for image classification, we will compare and evaluate the degree to which each strategy aids in preventing overfitting.


